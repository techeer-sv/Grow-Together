# 응답시간과 처리량

## 1.응답 시간

![image.png](attachment:f1163dce-75c0-4403-8226-3cf892c6a61f:image.png)

- **사용자가 요청을 보낸 순간부터 결과(응답)를 받는 데 걸린 시간**이에요.
- 예를 들어 앱에서 API 서버에 "내 요금 알려줘" 요청을 보냈다고 해봅시다.

흐름은 이렇게 됩니다:

1. 앱이 API 서버에 요청 보냄 (API 요청)
2. API 서버가 DB에 질의(SQL 실행)
3. DB가 결과를 API 서버로 반환
4. API 서버가 JSON 응답을 앱으로 반환

이 전체 과정(요청 → 처리 → 응답) 걸린 시간이 바로 응답 시간이에요.

![image.png](attachment:8ffc3dea-3966-4829-86f1-2ed63e6b34a0:image.png)

### 1-1. 응답 시간은 여러 단계가 합쳐진 것

클라이언트(앱, 브라우저)가 서버에 요청을 보내고 응답을 받을 때까지 여러 과정이 있습니다.

그림에 나온 응답 시간 구성 순서는 이렇게 나눌 수 있어요:

1. **서버에 연결 (Connection)**
    - 앱이 서버에 “연결선”을 뚫는 단계 (예: TCP 연결)
2. **데이터 전송 (Request Transmission)**
    - 앱이 요청 데이터를 서버로 보냄 (예: JSON 형식으로 보냄)
3. **서버 실행 (Processing)**
    - 서버가 요청을 받고 DB에 SQL 실행 → 결과 만들기
4. **클라이언트로 응답 전송 (Response Transmission)**
    - 서버가 만든 결과(JSON)를 다시 앱으로 보내줌

이 전체 시간이 모여서 **응답 시간**이 되는 거예요.

### 1-2. TTFB vs TTLB

응답 시간을 측정할 때 자주 쓰는 두 가지 개념이 있어요:

- **TTFB (Time To First Byte)**
    
    → 서버 응답 데이터 중 **첫 번째 바이트가 도착할 때까지 걸린 시간**
    
    (즉, 서버가 응답을 시작하기까지 얼마나 걸렸는가)
    
- **TTLB (Time To Last Byte)**
    
    → 서버 응답 데이터 중 **마지막 바이트까지 도착하는 데 걸린 시간**
    
    (즉, 응답 전체가 다 끝나기까지 걸린 총 시간)
    

### 1-3. 왜 구분할까?

- 데이터가 작으면 **TTFB ≈ TTLB** (거의 차이 없음)
- 데이터가 크거나 네트워크가 느리면
    
    **TTFB는 빨라도 TTLB가 훨씬 늦어질 수 있음**
    
    → 예: 파일 다운로드할 때 시작은 빨리 되지만 다 받으려면 오래 걸림
    

그래서 서버 성능을 볼 때는 **요청/응답 데이터 크기와 네트워크 상황**에 따라

TTFB, TTLB 중 어떤 지표가 더 중요한지 골라서 봐야 해요.

### 1-4.. 응답 시간이 중요한 이유

구글에서 발표한 유명한 연구가 있어요.

응답 시간이 조금만 늘어나도 사용자의 행동이 확 줄어든다는 거예요.

- **100ms(0.1초) 지연 → 검색 횟수 약 0.2% 감소**
- **400ms(0.4초) 지연 → 검색 횟수 약 0.6% 감소**

즉, **응답 시간이 조금만 늘어나도 사람들이 검색/사용을 덜 하게 된다**는 뜻이에요.

이게 누적되면 트래픽과 매출까지 줄어들 수 있습니다.

### 1-5. 응답 시간을 어떻게 쪼개서 보나?

응답 시간은 단순히 “API가 느리다”로 끝나지 않아요.

내부적으로는 여러 단계가 합쳐진 결과예요.

- **로직 수행 시간**: 코드 실행 (if문, 계산 등)
- **DB 연동 시간**: SQL 실행하는 데 걸린 시간
- **외부 API 연동 시간**: 다른 서버와 통신하는 데 걸린 시간
- **응답 데이터 생성/전송 시간**: JSON 만들고 보내는 시간

### 1-6. 실제 예시 (책에서 나온 측정 결과)

한 요청을 처리하는 데 총 348ms 걸렸는데, 그걸 쪼개보니:

- 외부 API 호출 1 (외부 네트워크): **186ms (53%)**
- 외부 API 호출 2 (내부 네트워크): **44ms (13%)**
- DB 연동 (SQL 실행 6회): **101ms (29%)**
- 로직 수행: **17ms (5%)**

→ 즉, 대부분의 시간은 **외부 API 호출과 DB 연동**에서 잡아먹고,

실제로 서버 코드 자체 실행(로직 수행)은 아주 적은 비중이라는 걸 알 수 있어요.

## 2.처리량

### 2-1. 처리량이란?

- 처리량 = 단위 시간(예: 1초)에 **시스템이 처리한 작업 수**
- 보통 두 가지 용어를 씁니다:
    - **TPS (Transactions Per Second)**: 초당 처리한 트랜잭션 수
    - **RPS (Requests Per Second)**: 초당 처리한 요청 수
- 책에서는 구분 없이 TPS를 사용합니다.

![image.png](attachment:6372d2a5-5871-4102-89ba-40a715145c88:image.png)

### 2-2. 그림 설명 (TPS 예시)

그림에서 화살표 하나는 “요청 1개”를 의미해요.

구간별로 몇 개의 요청이 처리됐는지를 세어 TPS를 계산합니다.

- **0~1초 구간**: 요청 3개 처리 → TPS = 3
- **0~2초 구간**: 요청 2개 처리 → TPS = 2
- **0~3초 구간**: 요청 2개 처리 → TPS = 2

즉, 특정 시간 동안 처리된 요청 수가 TPS가 되는 거예요.

### 2-3. 최대 TPS

- **최대 TPS = 서버가 동시에 처리할 수 있는 최대 요청 수**
- 예: 서버가 한 번에 5개 요청을 처리할 수 있다면 → 최대 TPS = 5
- 만약 요청이 몰려서 7개 들어오면?
    
    → 5개는 즉시 처리, 나머지 2개는 기다렸다가 처리해야 함
    

즉, **최대 TPS를 초과하면 요청이 지연되거나 대기**하게 됩니다.

### 2-4. 핵심 포인트

- *응답 시간(Response Time)**은 “한 요청이 얼마나 빨리 끝났는가”
- *처리량(Throughput, TPS)**은 “한 번에 얼마나 많은 요청을 감당할 수 있는가”

둘 다 성능을 평가하는 중요한 지표예요.

→ 빠른 서버는 **응답 시간도 짧고, TPS도 높습니다.**

![image.png](attachment:844f82cb-2a00-4d64-89b0-7468480c1f4d:image.png)

### 2-5.상황 예시

- 서버의 **최대 TPS = 5**라고 가정합니다. (즉, 동시에 최대 5개의 요청만 처리 가능)
- 그런데 **7개의 요청**이 동시에 들어오면?

👉 서버는 5개만 바로 처리하고,

나머지 2개는 **대기**해야 해요.

![image.png](attachment:933d3a84-0ef0-43fd-88e8-3636c747f7af:image.png)

### 2-6. 해결 방법 (TPS를 높이는 두 가지 방법)

1. **동시에 처리할 수 있는 요청 수 늘려서 대기시간 줄이기**

       → 서버 성능 확장, 멀티스레드/멀티프로세스, 서버 개수 늘리기(스케일 아웃)

1. **처리 시간 자체 줄여서 대기 시간 줄이기**
    
    → 코드 최적화, DB 쿼리 튜닝, 캐싱 도입
    

이 두 가지를 함께 적용하면 TPS를 높이고, 대기 시간을 줄일 수 있어요.

### 2-7.성능 개선 순서

- 막연히 “느리다” 해서 서버를 무작정 늘리면 안 됩니다 ❌
- 먼저 현재 서버의 **TPS와 응답 시간**을 정확히 측정해야 합니다.
- 그 다음 → 목표 TPS/응답 시간을 정하고 → 거기에 맞게 **효율적으로 성능 개선**을 해야 해요.

### 2-8. TPS 측정 방법

![image.png](attachment:37e2ec8a-5a46-4e2d-a7a6-1957e5f91a7d:image.png)

- TPS는 직접 측정할 수도 있지만, 보통 **모니터링 툴**을 씁니다.
- 예: **스카우터, 핀포인트, 뉴렐릭(New Relic)** 같은 툴
- 이런 걸 쓰면:
    - 현재 TPS
    - 과거 특정 시점 TPS
    - 응답 시간
        
        등을 실시간으로 확인할 수 있어요.
        

### 2-9. TPS 계산 방식

- 보통은 **짧은 구간(예: 5초)** 단위로 요청 수를 세고, 이를 나눠서 TPS를 구합니다.
    
    → “5초 동안 500건 처리했다 → TPS = 100”
    
- 더 정밀하게 보려면 엘라스틱서치로 **웹 서버 접근 로그**를 활용해 요청별 시간을 분석해서 TPS를 계산할 수도 있어요.

# 서버 성능 개선 기초

## 1.병목 지점

### 1-1. 성능 문제(병목 현상)는 언제 생기나?

- 서비스 초기에는 사용자 수가 적어서 성능 문제가 잘 안 보임
- 하지만 시간이 지나 **사용자와 트래픽이 늘어나면** → 응답이 느려지고 문제가 터짐
- 특히 **TPS(최대 처리량)**을 넘어서면 요청들이 대기하게 되고, 응답 시간은 기하급수적으로 늘어남

### 전형적인 증상

- 순간적으로 모든 요청의 응답이 느려짐 (10초 이상 걸리기도 함)
- 서버를 재시작하면 괜찮다가, 시간이 지나면 다시 느려짐
- 트래픽이 줄어들면 자연스럽게 응답 속도도 나아짐

즉, 병목(좁은 길목) 때문에 전체가 막히는 거예요.

### 1-2. 병목을 찾는 방법

- 성능 문제를 해결하려면 **먼저 병목 구간**을 찾아야 함
- 보통 DB 연동이나 외부 API 호출이 병목 구간인 경우가 많음
- 방법:
    - 모니터링 툴을 써서 실행 시간이 오래 걸린 부분을 추적
    - 로그를 보고 어떤 코드가 오래 걸렸는지 확인

## 2.수직 확장과 수평 확장

### 2-1. 수직 확장(Scale-up)

병목 지점을 찾았다면, 가장 빠르게 적용할 수 있는 방법은 **서버 자원 자체를 늘리는 것**이에요.

이를 **수직 확장(Scale-up)**이라고 부릅니다.

### 방법

- CPU 업그레이드 (더 빠른 CPU, 더 많은 코어)
- 메모리 증설 (RAM 추가)
- 디스크를 SSD로 교체 (I/O 속도 개선)

→ 이렇게 하면 서버 한 대의 **응답 속도**와 **처리량**이 즉시 좋아집니다.

### 2-2. 수직 확장의 한계

앞에서 봤듯이 **수직 확장(Scale-up, 서버 스펙 업그레이드)**은 즉각 효과가 있지만,

- CPU/메모리/디스크를 계속 늘리는 데는 **물리적 한계**가 있고
- 비용도 계속 올라갑니다.

그래서 일정 수준 이상 트래픽이 늘어나면 **수평 확장(Scale-out)**이 필요합니다.

### 2-3. 수평 확장(Scale-out)

- 서버를 여러 대 두고 요청을 나눠 처리하는 방식
- 예: 서버 1대 대신 서버 3대를 두고, **로드 밸런서(Load Balancer)**가 요청을 균등하게 분배

### 로드 밸런서란?

- 사용자의 요청이 한 서버에 몰리지 않도록 분산해주는 장치/시스템
- 방식:
    - **정적인 분배(**
        - **라운드 로빈** : 차례대로 돌아가면서 요청을 분배하는 방식
            - 예를 들어 서버가 3대 있다고 합시다.
            - 첫 번째 요청은 서버1, 두 번째 요청은 서버2, 세 번째 요청은 서버3으로 보냄.
            - 네 번째 요청이 들어오면 다시 서버1로 돌아가서 시작
        - **IP 해시** : 사용자의 IP 주소를 기준으로 서버를 정하는 방식
            - 클라이언트의 IP 주소를 해시(hash)라는 계산을 해서 특정 서버에 매칭.
            - 예를 들어 A 사용자의 IP는 계산 결과 서버2, B 사용자의 IP는 서버3으로 항상 고정.
            - 그래서 A 사용자가 다시 요청을 보내면 무조건 서버2로 가게 됨.
    - **동적인 분배**: 현재 서버 상태(부하, 연결 수 등)를 보고 가장 여유 있는 서버에 분배

👉 이렇게 하면 전체 서버 자원을 효율적으로 활용할 수 있고, **TPS를 높일 수 있음**.

### 3. 주의할 점

- 무작정 서버만 늘린다고 해결되지 않음
- 예: **DB가 병목**인데 웹 서버만 늘리면 → 결국 DB에 몰려서 성능 문제 여전함
- 즉, 어디가 병목인지 먼저 찾아내는 게 중요

## 3.커넥션 풀

### 3.1 커넥션풀이란?

DB는 요청마다 **연결 → 쿼리 실행 → 연결 종료** 과정을 거침

- 이 “연결/종료” 과정만 해도 네트워크 때문에 0.5~1초 걸릴 수 있음
- 쿼리는 0.01초 만에 끝나는데, 연결 과정에서 0.5초 쓰면 낭비가 엄청남
- 실제로 DB 처리 시간의 **80% 이상이 연결/종료 과정**에 쓰이기도 함

### 해결 방법: 커넥션 풀

- 미리 여러 개의 DB 연결을 만들어서 풀(pool)에 보관
- 요청이 오면 새로 연결하지 않고, 풀에서 연결을 빌려와서 바로 사용
- 사용 끝나면 다시 풀에 반환

👉 이렇게 하면 매번 연결/종료하는 시간을 크게 줄여 **응답 시간 단축 + TPS 향상** 효과를 얻을 수 있음.

### 3.2 커넥션 풀 설정

커넥션 풀은 다양한 설정을 제공함

- 커넥션 풀 크기
- 풀에 커넥션이 없을 떄 커넥션을 구할 때까지 대기할 시간
- 커넥션의 유지 시간

## 4.커넥션 풀 크기

### 4.1 커넥션 풀 크기 설정

커넥션 풀에는 “몇 개의 연결을 미리 만들어 둘까?”를 정해야 해요.

예를 들어:

- 커넥션 풀 크기 = 5
    
    ![image.png](attachment:3c4180dd-0b13-422c-ba9e-b49410d00b08:image.png)
    
- 동시에 6개의 요청이 들어옴 → 5개는 바로 연결 사용, 나머지 1개는 **대기**해야 해요.

📌 만약 요청이 많아지면 **풀 크기를 늘려야 응답 속도가 빨라져요.**

### 4-2. 예시 상황

### 예시 1: 풀 크기 = 5, 쿼리 실행 시간 = 0.1초

- 1초 동안 처리 가능한 요청 수:
    
    → **1 ÷ 0.1 × 5 = 50건**
    
- 동시에 50개 요청이 와도 **1초 이내 모두 처리 가능**.

### 예시 2: 풀 크기 = 5, 쿼리 실행 시간 = 0.5초

- 1초 동안 처리 가능한 요청 수:
    
    → **1 ÷ 0.5 × 5 = 10건**
    
- 동시에 50개 요청이 오면 **총 5초** 걸려야 처리 가능 → 응답 지연 발생.
- 해결책: **풀 크기를 50으로 늘리면** → 모든 요청을 0.5초 안에 처리 가능.

### 4-3. 커넥션 풀 동작 방식

- 풀 크기가 작으면, 요청이 몰릴 때 **대기 시간이 길어져 응답 속도 저하**.
- 풀 크기를 늘리면 동시에 더 많은 요청 처리 가능 → **TPS(초당 처리 수) 증가**.
- 하지만 **풀 크기를 너무 크게 잡으면** 불필요하게 리소스 점유 가능 → 최적 값 필요.

### 4-4. 최소/최대 크기 설정

- 일반적인 커넥션 풀은 **최소 크기**와 **최대 크기**를 설정 할 수 있음
- 예시: 최소 10, 최대 20 설
    - 동시 요청이 10개 이하일 때는 10개 커넥션 유지.
    - 만약 요청이 더 많아지면 → 20개까지 커넥션을 늘려 처리.
    - 요청이 줄어들면 다시 커넥션 수 줄임.

### 4-5.트래픽 패턴과 커넥션 풀 크기

- 트래픽은 보통 **증가했다가 감소하는 패턴**을 보임.
    - 예: 은행 서비스 → 낮에 트래픽 많음.
    - 게임 서비스 → 저녁·밤 시간에 트래픽 많음.
- 따라서:
    - **트래픽이 낮은 시간대**에는 최소 크기만 유지.
    - **트래픽이 높은 시간대**에는 최대 크기까지 확장하여 유지.
- 핵심 포인트
    - 순간적인 트래픽 폭주 → DB 연결 시간이 조금 느려져도 큰 문제 없음.
    - 지속적인 트래픽 폭주 → DB 연결 시간이 성능 저하의 주요 원인이 될 수 있음.
    - 그래서 **DB 커넥션 풀의 최대 크기를 충분히 크게 잡아두는 게 안정적**임.

### 4-6. 커넥션 풀 크기 조정의 한계

- 풀 크기를 늘리면 더 많은 요청을 동시에 처리할 수 있음.
- 하지만 무작정 늘리면 문제 발생:
    - DB 서버 CPU 사용률, 메모리 사용량 증가 → 오히려 성능 저하.
    - 결국 **DB 자체의 처리 한계**에 막힐 수 있음.
- 따라서 서버 상황(DB 사양, 트래픽 양)에 맞춰 최적화해야 함.

## 5. 커넥션 대기 시간

### 5-1. 대기 시간(Wait Time)이란?

- **DB 커넥션 풀이 모두 사용 중일 때, 새로운 요청이 커넥션을 얻기 위해 기다릴 수 있는 최대 시간**
- 즉, "빈 커넥션이 날 때까지 몇 초 기다릴 건가?"를 정하는 설정.

👉 설정한 대기 시간 안에 커넥션을 못 얻으면 → **에러 발생(DB 연결 실패).**

### 5-2. 대기 시간이 길면?

- 요청은 기다리긴 하지만, 그만큼 **응답 시간이 길어짐**.
- 예를 들어 `대기 시간 = 30초` → 최악의 경우, 사용자는 응답을 30초 넘게 기다릴 수도 있음.
- 많은 요청이 동시에 몰리면, 계속 대기 → 서버가 처리해야 할 요청 수가 눈덩이처럼 불어남.

### 5-3. 대기 시간이 짧으면?

- 요청이 커넥션을 못 얻으면 빠르게 **에러를 반환**.
- 예: `대기 시간 = 1초` → 1초 안에 못 얻으면 즉시 실패 응답.
- 장점: 서버가 무한히 쌓이지 않고, **트래픽을 안정적으로 처리 가능**.
- 단점: 사용자는 에러를 볼 확률 ↑

### 5-4. 예시 상황

![image.png](attachment:fd58c3eb-1947-4d7f-b2d5-b4fd27a2c6a0:image.png)

- 커넥션 풀 크기: **10개**
- 대기 시간: **30초**
- 동시에 요청: **30개**
- 쿼리 실행 시간: **10초**

➡️ 10개 요청은 바로 실행됨.

➡️ 나머지 20개 요청은 대기(최대 30초).

➡️ 만약 일부 사용자가 중간에 요청을 취소하고 다시 요청하면, 서버가 계속 새로운 요청을 처리해야 해서 **부하가 더 커짐**.

### 5. 대기 시간을 짧게 두면?

- 예: 대기 시간 = 1초
- 30개 요청이 동시에 들어와도 → 10개는 실행, 나머지 20개는 1초 안에 실패 처리.
- 결과: 서버는 **동시에 10개만 안정적으로 처리**, 불필요한 대기 줄이기 가능.

**핵심 요약**

- **대기 시간 길게 설정** → 요청은 안 끊기지만 응답 느려지고 서버 과부하 위험.
- **대기 시간 짧게 설정** → 빠른 에러 응답, 서버는 안정적으로 유지.
- 서비스 성격에 따라 다르지만 보통 **0.5~3초 정도**로 짧게 설정하는 게 좋음.

## 6.최대 유휴시간, 유효성 검사, 최대 유지 시간

### 6-1. 최대 유휴 시간 (Idle Timeout)

- 의미: **사용되지 않는 커넥션을 풀에 유지할 수 있는 최대 시간**
- 예: 최대 유휴 시간이 `30분` → 30분 동안 사용되지 않은 커넥션은 풀에서 제거됨.
- 이유: DB가 오래 사용되지 않는 커넥션을 끊어버리면 풀에 **죽은 커넥션**이 남아 에러 발생 가능 → 따라서 주기적으로 정리 필요.

### 6-2. 유효성 검사 (Validation / Health Check)

- 의미: 커넥션이 정상적으로 살아있는지 주기적으로 점검하는 기능.
- 방법:
    - 풀 구현체가 자동으로 체크하거나,
    - 직접 쿼리를 실행해 확인 (`SELECT 1` 같은 단순 쿼리).
- 목적: **죽은 커넥션을 미리 제거**해서 애플리케이션이 에러 없이 안정적으로 동작하게 함.

### 6-3. 최대 유지 시간 (Max Lifetime)

- 의미: **커넥션이 생성된 시점부터 유지될 수 있는 최대 시간**
- 예: `4시간`으로 설정 → 커넥션이 4시간이 지나면 유효하더라도 풀에서 제거됨.
- 이유: 오래된 커넥션은 DB 측에서 끊길 수도 있기 때문에, 풀에서 강제로 정리.

### 6-4. Memo (핵심 요약)

- **최대 유휴 시간**과 **최대 유지 시간**을 무제한으로 두지 말 것.
- DB 설정(세션 타임아웃 등)과 맞춰서 적절히 지정해야 안정적 운영 가능.

## 7.캐시

### 7-1. 서버 캐시란?

- DB 서버 확장(더 좋은 장비, 더 많은 장비)은 비용이 많이 듦.
- 캐시(Cache)를 사용하면 DB 서버 확장 없이도 **응답 속도와 처리량을 개선**할 수 있음.
- 캐시는 데이터를 **(키, 값)** 형태로 저장 → 같은 요청이 오면 DB 대신 캐시에서 바로 응답.

### 7-2. 동작 방식 (그림 2.12 참고)

![image.png](attachment:d458b6de-d387-4ffc-9bdc-564673ca9b75:image.png)

1. 요청이 들어오면 → 캐시에서 해당 데이터(값)를 먼저 찾음.
2. 캐시에 데이터가 있으면 → 바로 사용 (빠름).
3. 캐시에 데이터가 없으면 → DB에서 조회 후, 캐시에 저장하고 사용.
4. 이후 동일한 요청이 오면 → 캐시에 저장된 데이터를 사용.

➡️ 자주 쓰는 데이터를 캐시에 저장해 두면 DB 부하를 줄이고, 응답 속도도 빨라짐.

### 7-3. 캐시의 장점

- DB보다 읽기 속도가 훨씬 빠름 → 응답 시간 단축.
- DB 부하 감소 → 서버 안정성 ↑
- 단순 조회뿐만 아니라 계산 결과, 외부 API 호출 결과도 캐시에 저장해 재사용 가능.

### 7-4. 캐시 키 선택

- 캐시는 `(키, 값)` 구조라서 **키를 어떻게 정하느냐가 중요**.
- 예시:
    - 게시글 상세 조회 → `articles:번호`
    - 인기글 TOP 10 → `articles:hot10`
- 키가 겹치지 않도록 주의해야 함.

## 8.적중률과 삭제 규칙

### 8-1. 캐시 적중률 (Hit Rate)

- **공식**: 캐시 적중률 = (캐시에 있었던 조회 성공 횟수) ÷ (전체 조회 시도 횟수)
- 예시: 데이터를 100번 조회했는데 87번 캐시에서 찾았다면 → 적중률 0.87 (=87%)

👉 **적중률이 높을수록 DB를 덜 조회하므로 응답 시간 단축 + DB 부하 감소**

### 8-2. 캐시에 얼마나 저장할까?

- 캐시에 **많이 저장할수록 적중률 ↑**
- 예: 5,000개 상품 중 100개만 캐시에 저장 → 2%
    
    4,500개 저장 → 90%
    
    5,000개 전부 저장 → 100%
    

하지만… ❗ 캐시는 메모리를 쓰기 때문에 무한정 저장 불가 → 관리 필요

### 8-3. 캐시 삭제(교체) 규칙

캐시가 가득 차면 오래된 데이터나 덜 쓰는 데이터를 지워야 새 데이터를 넣을 수 있음.

대표적인 규칙:

1. **LRU (Least Recently Used)**
    - 가장 오래 사용되지 않은 데이터 삭제
    - (예: 최근에 안 본 유튜브 영상부터 삭제)
2. **LFU (Least Frequently Used)**
    - 가장 사용 빈도가 적은 데이터 삭제
    - (예: 사람들이 거의 안 찾는 상품 삭제)
3. **FIFO (First In First Out)**
    - 가장 먼저 들어온 데이터부터 삭제
    - (예: 오래된 순으로 정리)

### 8-4. 추가 관리: 만료 시간(Expiration)

- 오래된 데이터는 쓸모가 없을 수도 있으니 **유효기간(예: 1시간, 하루)** 설정
- 시간이 지나면 캐시에서 자동 삭제됨 → 메모리를 효율적으로 사용 가능

## 9.로컬 캐시와 리모트 캐시

![image.png](attachment:506eb6c7-d8d4-4dd6-89b0-e87565abe5ab:image.png)

### 9-1.로컬 캐시 (Local Cache)

- **정의**: 서버 프로세스와 같은 메모리에 캐시 데이터를 저장. (인메모리 캐시라고도 부름)
- **대표 기술**: Caffeine(Java), go-cache(Go), node-cache(Node.js)
- **장점**
    - **빠름** → 메모리 접근이므로 속도가 매우 빠름.
    - 외부 네트워크 통신이 필요 없어 **구조 단순**.
- **단점**
    - **저장 용량 제한** → 서버 메모리 크기에 한정됨.
    - **서버 재시작 시 데이터 유실** → 메모리에 있던 캐시 데이터가 모두 사라짐.

### 9-2.리모트 캐시 (Remote Cache)

- **정의**: 별도의 캐시 서버(예: Redis, Memcached)를 두고, 여기에 캐시 데이터를 저장.
- **장점**
    - **확장성 좋음** → 여러 대 서버에 걸쳐 대규모 데이터 저장 가능.
    - 서버 재시작해도 캐시 데이터가 유지됨.
- **단점**
    - **느림** → 네트워크 통신 필요해서 로컬 캐시보다 속도 떨어짐.
    - **구조 복잡** → 별도 캐시 서버 운영해야 함.

### 9-3.언제 로컬 vs 리모트?

- **로컬 캐시 적합**
    - 데이터가 작고 변경이 자주 없음.
    - 서버 메모리에 다 담을 수 있음.
    - 예: 인기글 Top 10, 자주 조회되는 설정 값 등.
- **리모트 캐시 적합**
    - 데이터 양이 크고, 자주 변경됨.
    - 트래픽이 많아 여러 서버에서 동일한 캐시 데이터를 공유해야 함.
    - 예: 대형 쇼핑몰 상품 데이터, 수백만 건의 사용자 정보 등.

## 10.캐시 사전 적재 (Pre-warming)

- **트래픽이 순간적으로 급증하는 상황**을 대비해서, **미리 데이터를 캐시에 넣어두는 방법**.
- 예시:
    - G 통신사 사용자는 300만 명.
    - 매달 특정일 요금 정보 갱신 → 동시에 150만 명이 앱을 열어 요금 확인.
    - 캐시에 미리 데이터를 넣어두면 → 사용자가 조회할 때 바로 캐시에서 응답 → DB 부하 방지 & 빠른 응답.
- 장점:
    - 캐시 적중률(히트율)을 99% 이상으로 유지 가능.
    - 갑작스러운 트래픽 증가에도 안정적으로 서비스 제공.
    - DB 과부하를 막고 전체 응답 시간 단축.

## 11.캐시 무효화 (Cache Invalidation)

- **캐시에 있는 데이터가 더 이상 유효하지 않을 때 캐시에서 삭제**해야 함.
- 원본 데이터(DB 값)가 바뀌었는데 캐시가 갱신되지 않으면 **사용자가 오래된 잘못된 정보**를 보게 됨.
- 무효화 전략:
    1. **즉시 무효화**: 데이터 변경 시 바로 캐시 삭제 (예: 가격, 게시글 같은 민감한 데이터).
    2. **주기적 갱신**: 자주 바뀌는 데이터는 일정 시간마다 자동으로 갱신 (예: 인기글 목록).
- 예시:
    - 인기글 TOP10 캐시 → 10분마다 새로 갱신.
    - 더 빠른 최신성이 필요하다면 → 1분으로 줄일 수도 있음.
- 중요 포인트:
    - **변경이 민감한 데이터** → 바로 무효화.
    - **변경이 자주 없는 데이터** → 주기적 갱신.
    - 로컬 캐시는 서버 재시작 시 데이터가 사라질 수 있으므로, 중요한 데이터는 리모트 캐시에 저장하는 게 안정적.

## 12.가비지 컬렉터와 메모리 사용

### 12-1. 가비지 컬렉터(GC)란?

- 자바, Go, 파이썬 같은 언어는 **GC**를 사용.
- GC는 "더 이상 쓰이지 않는 메모리(객체)"를 자동으로 찾아서 정리하는 기능.
- 개발자가 일일이 메모리 관리할 필요가 없어서 편리함 👍

### 12-2. GC의 장단점

장점:

- 개발자가 직접 메모리 해제 안 해도 됨 → 실수 줄어듦.
- 메모리 관리가 자동이라 편리.

단점:

- GC 실행되는 동안 애플리케이션이 잠시 멈출 수 있음 → 이를 **Stop-The-World**라고 부름.
- 메모리를 너무 많이 쓰면 GC 실행이 잦아져 성능 저하 발생.

### 12-3. 메모리 크기와 GC

- 메모리 크기가 크면 → GC 실행 횟수는 줄지만, 한 번 실행할 때 시간이 오래 걸림.
- 메모리 크기가 작으면 → GC 실행 횟수는 늘지만, 한 번 실행할 때 빨리 끝남.
- 따라서 애플리케이션 성격에 맞게 **적절한 메모리 크기 설정**이 중요함.

### 12-4. 객체 생성/조회 시 주의점

- 한 번에 너무 많은 객체나 데이터를 메모리에 올리면 OutOfMemory 위험 ⚠️
    
    → 예: 한 번에 10만 개 게시글 다 불러오기 ❌
    
- 대신 **필요한 범위만 제한해서 조회**하는 게 좋음.
    
    → 예: 최근 3개월 데이터까지만 조회 가능하게 하기.
    

### 12-5. 파일 처리 시 메모리 최적화

- `readAllBytes()` 같이 파일을 한 번에 메모리에 올리면 큰 메모리가 필요함.
- 파일 다운로드 같은 기능은 스트림(stream)을 활용해 조금씩 읽고 처리해야 메모리 절약 가능.

### 12-6. 실제 사례 (대량 데이터로 서버 중지)

- 어떤 회사 서비스에서 **엑셀 다운로드 기능** 때문에 문제가 생김.
- 수백만 건 데이터를 한 번에 메모리에 올려서 엑셀 파일을 만들다 보니 → 서버 메모리 부족으로 다운.
- 개선 방법:
    1. **스트림 방식**으로 엑셀을 바로바로 파일에 기록 → 메모리에 쌓이지 않음.
    2. DB에서 조회 결과도 한 번에 다 가져오지 않고, 일정 단위(배치)로 나눠서 처리.

→ 이 방법으로 메모리 사용량 대폭 줄이고 서버 안정화 성공.

### 12-7. 디스코드가 러스트(Rust)로 간 이유

- 디스코드는 성능 최적화를 위해 다양한 기술을 적극 도입하는 회사.
- 과거에 Go 언어 기반 서버를 사용했는데, 문제는 **GC(가비지 컬렉션)** 때문.
    - GC가 실행될 때마다 응답 시간이 늘어나고 CPU 사용률이 크게 증가 → 성능 저하.
- 해결 방법: **러스트(Rust)** 로 전환.
    - 러스트는 GC가 없어서 이런 성능 저하 문제가 없음.
    - 대신 메모리 관리 책임은 개발자에게 있음.
- 결론: 트래픽이 많고 성능이 중요한 서비스라면 GC 없는 언어도 고려할 가치 있음.

## 13. 응답 데이터 압축

서버에서 응답 시간 = **데이터 전송 시간** 포함.

응답 속도에 영향을 주는 요인 2가지:

1. **네트워크 속도** (사용자 환경)
    - 느리면 응답이 오래 걸림. (예: 와이파이 불안정)
2. **전송 데이터 크기**
    - 10KB 파일 전송은 빠름, 1GB 파일 전송은 오래 걸림.

👉 서버는 네트워크 속도는 못 바꾸지만, **데이터 크기는 줄일 수 있음 → 압축**

### 압축 예시

- HTML, CSS, JS, JSON 같은 텍스트 파일은 압축률이 높음.
- gzip 같은 알고리즘으로 압축하면 데이터 크기를 **70% 이상 줄일 수 있음**.
- 전송 데이터가 줄어들면 전송 시간도 줄고, 결국 **응답 시간도 짧아짐**.

## 14.정적 자원과 브라우저 캐시

### 14-1.정적 자원 vs 동적 자원

- **동적 자원**: 사용자 요청할 때마다 결과가 바뀌는 데이터
    
    → 예: 상품 목록, 상품 상세 JSON, HTML 응답 등
    
- **정적 자원**: 한 번 만들어지면 잘 안 바뀌는 데이터
    
    → 예: 이미지, JS, CSS 같은 파일
    
    ![image.png](attachment:3df772b0-e13a-4143-87b1-46510762da51:image.png)
    

⚡ 쇼핑몰 사이트 첫 페이지 트래픽의 80~90%는 **정적 자원(특히 이미지+JS)** 이 차지함.

### 14-2.문제점

- 사용자가 같은 페이지에 여러 번 들어갈 때 **JS, CSS, 이미지 등을 계속 다운로드**하면 서버에 부하 ↑, 네트워크 트래픽 ↑, 비용 ↑, 사용자 체감 속도 ↓

### 14-3.해결 방법: 브라우저 캐시 활용

- **HTTP Cache-Control / Expires 헤더** 사용 → 브라우저가 정적 자원을 **일정 시간 동안 재사용**하게 만듦
- 예시:
    
    ```
    Cache-Control: max-age=60
    ```
    
    → 60초 동안 브라우저가 같은 파일 요청 시 서버에 안 가고, 캐시에서 꺼내 씀
    
- 즉, **같은 이미지/JS 파일은 브라우저가 저장해 두고 반복 요청 시 다시 다운로드하지 않음** → 서버 부하 줄고, 응답 속도 빨라짐.

## 15.정적 자원과 CDN

### 1. 브라우저 캐시 한계

- 브라우저 캐시를 쓰면 **네트워크 트래픽을 줄일 수 있음**.
- 하지만 브라우저 단위로만 동작하기 때문에,
    
    동시에 많은 사용자가 접속하면 순간적으로 **이미지, JS, CSS 전송량 폭증** → 네트워크 응답 시간이 느려짐.
    
- 예시: 고속도로 1~2대 차는 문제 없지만, 수십 대가 몰리면 정체 발생

### 2. CDN(Content Delivery Network)

- CDN은 전 세계 여러 지역에 서버(엣지 서버)를 두고 **사용자와 가까운 서버에서 콘텐츠 제공**.
- 대표 서비스: **CloudFront(AWS), Akamai, Cloudflare**
- 구조:
    - 사용자는 가까운 **CDN 서버**에서 데이터 받음 → 더 빠른 속도
    - 오리진(원본 서버)에는 캐시되지 않은 요청만 전달됨 → **원본 서버 부하 감소**

 장점

- 빠른 응답 속도 (지리적 거리 단축)
- 원본 서버 트래픽 줄어듦
- 브라우저 캐시보다 훨씬 효율적

### 3. 주의 사항

- 정적 파일 관리할 때 **크기** 주의 ⚠️
    - 예: 30MB 이미지를 실수로 올리면 → 비용 폭증, 트래픽 급증
- 계약 조건에 따라 트래픽 초과 요금이 붙기도 함.
- 해결책 → 서버나 스토리지에서 **업로드 크기 제한** 설정 필요.

## 16.대기 처리

- **문제 상황**
    - 갑자기 트래픽이 폭주하는 경우가 있음 (예: 콘서트 예매 시작 → 몇 분 만에 매진).
    - 동시에 많은 사용자가 몰리면 서버와 DB가 감당 못 해서 터질 수 있음.
- **잘못된 해결 방법**
    - 순간 트래픽 맞추려고 서버·DB 성능을 과도하게 높이는 방법.
    - 하지만 평소에는 필요 없는 자원浪費 → 유지비만 커짐 (DB 비용 ↑).
- **현실적인 해결 방법: 대기 처리(Queueing)**
    - **모든 요청을 동시에 받지 않고** → 시스템이 감당 가능한 만큼만 처리.
    - 나머지는 대기열(Queue)에 쌓아두고 순서대로 처리.
    - 은행 창구에 줄 서는 것과 같은 원리.
    - 예: 사용자가 대기 화면(“앞에 2000명이 있습니다”)을 보고 순서를 기다림.
- **장점**
    - 서버가 폭주로 다운되는 것 방지.
    - 트래픽을 흡수하면서 서비스 안정성 유지.
    - 불필요하게 DB 자원을 크게 늘리지 않아도 됨.
